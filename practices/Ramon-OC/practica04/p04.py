# -*- coding: utf-8 -*-
"""P04.ipynb

Automatically generated by Colab.

"""
# paqueterias necesarias para el correcto funcionamiento de la práctica
! pip install elotl

import requests
import re
import math
from collections import Counter
from nltk.corpus import brown
import elotl.corpus

brown_words = [word for word in brown.words() if re.match("\w", word)]
axolotl = elotl.corpus.load("axolotl")
axolotl_words = [word for row in axolotl for word in row[1].lower().split()]

# URLs de .bpe
brown_bpe_url = "https://raw.githubusercontent.com/Ramon-OC/cl-2024-2-lab/practica04/practices/Ramon-OC/practica04/brown.bpe"
axolotl_bpe_url = "https://raw.githubusercontent.com/Ramon-OC/cl-2024-2-lab/practica04/practices/Ramon-OC/practica04/axolotl.bpe"

def download_file_from_github(url: str) -> str:
    response = requests.get(url)
    response.raise_for_status()
    return response.text

brown_bpe_content = download_file_from_github(brown_bpe_url)
axolotl_bpe_content = download_file_from_github(axolotl_bpe_url)

brown_bpe = brown_bpe_content.split()
axolotl_bpe = axolotl_bpe_content.split()

def compute_entropy(text: list[str]) -> float:
    word_frequencies = Counter(text)
    total_words = len(text)
    word_probabilities = [count / total_words for count in word_frequencies.values()]
    entropy = 0
    for probability in word_probabilities:
        entropy -= probability * math.log2(probability)
    return entropy

# Imprimir en pantalla:
#   Entropía de axolotl word-base y bpe
#   Entropía del brown word-base y bpe
print("Entropía del brown word-base y bpe:")
print("word-base: ", round(compute_entropy(brown_words), 2))
print("BPE: ", round(compute_entropy(brown_bpe), 2))

print("\nEntropía de axolotl word-base y bpe:")
print("word-base: ", round(compute_entropy(axolotl_words), 2))
print("BPE: ", round(compute_entropy(axolotl_bpe), 2))

'''
¿Aumento o disminuyó la entropía para los corpus?

  En ambos casos con BPE, para brown y axolotl la entropía disminuyó. En este
  caso cada palabra se trata como un token único.


¿Cómo influye la tokenización en la entropía de un texto?

  En Byte Pair Encoding cuando se dividen las palabras en unidades más pequeñas
  la distribución de tokens es más uniforme reduciendo de esta manera la
  entropía.

  En Word level al dividir el texto en palabras individuales implica una mayor
  variabilidad en la distribución de palabras.
'''

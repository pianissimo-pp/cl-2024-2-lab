# -*- coding: utf-8 -*-
"""Practica05.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ABNr7bM-xddptzREYAbg9lmdeiDxs4uo

# Practica 05

Alumno: David Pérez Jacome \\
Número de Cuenta: 316330420

**Actividades**


Hay varios métodos que podemos aplicar para reduccir la dimensionalidad de nuestros vectores y asi poder visualizar en un espacio de menor dimensionalidad como estan siendo representados los vectores.
  - PCA
  - T-SNE
  - SVD

1. Entrenar un modelo word2vec
  - Utilizar como corpus la wikipedia como en la practica
  - Adaptar el tamaño de ventana y corpus a sus recursos de computo
  - Ej: Entrenar en colab con ventana de 5 y unas 100k sentencias toma ~1hr

2. Aplicar los 3 algoritmos de reduccion de dimensionalidad
  - Reducir a 2d
  - Plotear 1000 vectores de las palabras más frecuentes

3. Analizar y comparar las topologías que se generan con cada algoritmo
  - ¿Se guardan las relaciones semánticas? si o no y ¿porqué?
  - ¿Qué método de reducción de dimensaionalidad consideras que es mejor?
"""

!pip install gensim wikipedia-api nltk

import gensim
import wikipediaapi
import requests
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from gensim.models import Word2Vec
import random
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA, TruncatedSVD
from sklearn.manifold import TSNE
from collections import Counter
from bs4 import BeautifulSoup

# Descargar recursos de nltk
nltk.download('punkt')

# Especificar el User-Agent
USER_AGENT = "MyWord2VecApp/1.0 (john_doe@example.com)"

# Función para obtener el contenido de una página de Wikipedia
def get_wikipedia_page_content(page_title):
    url = f"https://en.wikipedia.org/api/rest_v1/page/html/{page_title}"
    headers = {"User-Agent": USER_AGENT}
    response = requests.get(url, headers=headers)
    return response.text

# Ejemplo: extraer sentencias de una categoría de Wikipedia (puede ser ajustado según la necesidad)
category_title = "Category:Machine_learning"
category_content = get_wikipedia_page_content(category_title)

# Limpiar el HTML utilizando BeautifulSoup
soup = BeautifulSoup(category_content, 'html.parser')
wiki_text = soup.get_text()

# Tokenizar el texto en sentencias y palabras
sentences = [word_tokenize(sentence) for sentence in sent_tokenize(wiki_text)]

# Entrenar el modelo Word2Vec
window_size = 5
word2vec_model = Word2Vec(sentences, vector_size=100, window=window_size, min_count=2, workers=4)

# Guardar el modelo
word2vec_model.save("word2vec_wikipedia.model")

# Cargar el modelo entrenado
word2vec_model = Word2Vec.load("word2vec_wikipedia.model")

# Obtener las palabras más frecuentes
words = list(word2vec_model.wv.index_to_key)
word_freq = Counter(words)
common_words = word_freq.most_common(1000)
common_words = [word for word, _ in common_words]

# Obtener los vectores correspondientes a las palabras más frecuentes
word_vectors = np.array([word2vec_model.wv[word] for word in common_words])

# Aplicar PCA
pca = PCA(n_components=2)
word_vectors_pca = pca.fit_transform(word_vectors)

# Aplicar t-SNE con un valor de perplexity adecuado
tsne = TSNE(n_components=2, perplexity=5, random_state=42)
word_vectors_tsne = tsne.fit_transform(word_vectors)


# Aplicar SVD
svd = TruncatedSVD(n_components=2)
word_vectors_svd = svd.fit_transform(word_vectors)

# Plotear los resultados
def plot_vectors(vectors, words, title):
    plt.figure(figsize=(15, 10))
    plt.scatter(vectors[:, 0], vectors[:, 1], edgecolors='k', c='r')
    for i, word in enumerate(words):
        plt.text(vectors[i, 0], vectors[i, 1], word)
    plt.title(title)
    plt.show()

plot_vectors(word_vectors_pca, common_words, "PCA")
plot_vectors(word_vectors_tsne, common_words, "t-SNE")
plot_vectors(word_vectors_svd, common_words, "SVD")

# Análisis de las relaciones semánticas y comparación de los métodos
print("\nAnálisis de las topologías generadas:")
print("¿Se guardan las relaciones semánticas?")
print("Sí, en general las relaciones semánticas se mantienen, aunque la efectividad puede variar según el método.")
print("\n¿Qué método de reducción de dimensionalidad consideras que es mejor?")
print("Depende del propósito:")
print("- PCA: Bueno para capturar la varianza global en datos lineales.")
print("- t-SNE: Excelente para capturar estructuras locales y agrupaciones.")
print("- SVD: Similar a PCA, útil en sistemas de recomendación y datos dispersos.")
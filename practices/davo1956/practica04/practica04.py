# -*- coding: utf-8 -*-
"""Practica04.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ii1JIB_KfA9s-qxw2wQ8SJrW4GzxlK3G

# Practica 04

Alumno: David Pérez Jacome \\
Número de Cuenta: 316330420

Importamos los corpus que usaremos en la practica, tanto brown como axolotl
"""

!pip install elotl
import nltk
from nltk.corpus import brown
nltk.download('brown')
import elotl.corpus
axolotl = elotl.corpus.load("axolotl")

"""1. Calcular la entropía de dos textos (Axolotl y Brown):
  - Calcular para los textos tokenizados word-level
  - Calcular para los textos tokenizados con BPE
    - Tokenizar con la biblioteca  subword-nmt

2. Imprimir en pantalla:
  - Entropía de axolotl word-base y bpe
  - Entropía del brown word-base y bpe

3. Responder las preguntas:
  - ¿Aumento o disminuyó la entropia para los corpus?
    - axolotl
    - brown
  - ¿Qué significa que la entropia aumente o disminuya en un texto?
  - ¿Como influye la tokenizacion en la entropía de un texto?

"""

!pip install elotl subword-nmt nltk

import nltk
from nltk.corpus import brown
from collections import Counter
import numpy as np
import elotl.corpus
from subword_nmt import learn_bpe, apply_bpe
import io

# Descargar el corpus brown
nltk.download('brown')

# Cargar el corpus axolotl
axolotl = elotl.corpus.load("axolotl")

# Función para calcular la entropía
def entropy(text):
    """Calculate the entropy of a given text."""
    counter = Counter(text)
    total_count = sum(counter.values())
    probabilities = [count / total_count for count in counter.values()]
    return -sum(p * np.log2(p) for p in probabilities)

# Tokenización a nivel de palabras
brown_words = brown.words()
axolotl_words = [word for sublist in axolotl for word in sublist]  # Aplanar la lista

# Calcular la entropía a nivel de palabras
brown_word_entropy = entropy(brown_words)
axolotl_word_entropy = entropy(axolotl_words)

# Funciones para tokenización con BPE
def tokenize_with_bpe(text, num_operations=10000):
    """Tokenize text using Byte Pair Encoding (BPE)."""
    # Prepare text for BPE learning
    text_str = ' '.join(text)
    with io.StringIO(text_str) as text_file:
        bpe_codes = io.StringIO()
        learn_bpe.learn_bpe(text_file, bpe_codes, num_operations)

    bpe_codes.seek(0)
    bpe = apply_bpe.BPE(bpe_codes)

    # Apply BPE to the text
    tokenized_text = [bpe.process_line(word) for word in text]
    return tokenized_text

# Tokenizar con BPE
brown_bpe_tokens = tokenize_with_bpe(brown_words)
axolotl_bpe_tokens = tokenize_with_bpe(axolotl_words)

# Calcular la entropía con BPE
brown_bpe_entropy = entropy(brown_bpe_tokens)
axolotl_bpe_entropy = entropy(axolotl_bpe_tokens)

# Imprimir los resultados
print(f"Entropía de Axolotl (word-level): {axolotl_word_entropy}")
print(f"Entropía de Axolotl (BPE): {axolotl_bpe_entropy}")
print(f"Entropía de Brown (word-level): {brown_word_entropy}")
print(f"Entropía de Brown (BPE): {brown_bpe_entropy}")

# Responder las preguntas
def analyze_entropy_change(word_entropy, bpe_entropy):
    if bpe_entropy > word_entropy:
        return "aumentó"
    else:
        return "disminuyó"

axolotl_entropy_change = analyze_entropy_change(axolotl_word_entropy, axolotl_bpe_entropy)
brown_entropy_change = analyze_entropy_change(brown_word_entropy, brown_bpe_entropy)

print("\n¿Aumento o disminuyó la entropía para los corpus?")
print(f"- Axolotl: La entropía {axolotl_entropy_change}.")
print(f"- Brown: La entropía {brown_entropy_change}.")

print("\n¿Qué significa que la entropía aumente o disminuya en un texto?")
print("La entropía mide la incertidumbre o la cantidad de información en un texto. Un aumento en la entropía indica mayor diversidad o menos redundancia en el texto, mientras que una disminución en la entropía indica mayor redundancia o menos diversidad.")

print("\n¿Cómo influye la tokenización en la entropía de un texto?")
print("La tokenización afecta la entropía porque cambia la representación del texto. La tokenización a nivel de palabras puede tener menos tokens únicos y más repetición, mientras que la tokenización con BPE genera más tokens únicos al dividir palabras en subunidades, lo que puede aumentar la entropía al reducir la redundancia.")